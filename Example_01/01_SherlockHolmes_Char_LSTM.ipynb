{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from tensorflow.keras.layers import TextVectorization, LSTM, Embedding, Dropout, Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 17:05:53.719110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:05:53.727257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:05:53.727707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:05:53.729131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compute_capability': (7, 5), 'device_name': 'NVIDIA GeForce GTX 1650'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.get_device_details(gpu_devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 26 17:05:54 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 516.59       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P5     6W /  N/A |    162MiB /  4096MiB |     27%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 7\n",
    "SEQ_LEN = 512\n",
    "VAL_SIZE = 0.05\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "SHUFFLE_BUFFER = 1_000\n",
    "EMBEDDING_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'ber right, you\\n had not heard the name of Professor James Moriarty, who had one\\n of the great brains of the century. Just give me down my index of\\n biographies from the shelf.”\\n\\n He turned over the pages lazily, leaning back in his chair and\\n blowing great clouds from his cigar.\\n\\n “My collection of M’s is a fine one,” said he. “Moriarty himself\\n is enough to make any letter illustrious, and here is Morgan the\\n poisoner, and Merridew of abominable memory, and Mathews, who\\n knocked out my left canine in the w'\n",
      "Output: a\n",
      "\n",
      "Input: 'ommitted an\\nindiscretion.”\\n\\n“I was mad—insane.”\\n\\n“You have compromised yourself seriously.”\\n\\n“I was only Crown Prince then. I was young. I am but thirty now.”\\n\\n“It must be recovered.”\\n\\n“We have tried and failed.”\\n\\n“Your Majesty must pay. It must be bought.”\\n\\n“She will not sell.”\\n\\n“Stolen, then.”\\n\\n“Five attempts have been made. Twice burglars in my pay ransacked her\\nhouse. Once we diverted her luggage when she travelled. Twice she has\\nbeen waylaid. There has been no result.”\\n\\n“No sign of it?”\\n\\n“Absolutely no'\n",
      "Output: n\n",
      "\n",
      "Input: ' into the secrets of\\n private individuals, when you rake up family matters which are\\n better hidden, and when you incidentally waste the time of men\\n who are more busy than yourself. At the present moment, for\\n example, I should be writing a treatise instead of conversing\\n with you.”\\n\\n “No doubt, Doctor; and yet the conversation may prove more\\n important than the treatise. Incidentally, I may tell you that we\\n are doing the reverse of what you very justly blame, and that we\\n are endeavouring to prevent anyt'\n",
      "Output: h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "books = Path(\"../Data/Text/Sherlock_Holmes/\").rglob(\"*.txt\")\n",
    "\n",
    "for book in books:\n",
    "    with book.open('r', encoding = 'utf-8') as book_file:\n",
    "        book_data = book_file.read()\n",
    "        book_data = re.sub(\"[ ]+\", \" \", book_data)\n",
    "        char_len = len(book_data)\n",
    "\n",
    "        for i in range(0, char_len - SEQ_LEN):\n",
    "            X.append(book_data[i : i + SEQ_LEN])\n",
    "            y.append(book_data[i + SEQ_LEN])\n",
    "\n",
    "for i in np.random.randint(0, len(X), 5):\n",
    "    print(f'Input: {X[i]!r}')\n",
    "    print(f'Output: {y[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1762106, 1762106)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1674000, 1674000, 88106, 88106)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = VAL_SIZE, random_state = RANDOM_STATE)\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 295464),\n",
       " ('e', 155851),\n",
       " ('t', 112814),\n",
       " ('a', 101599),\n",
       " ('o', 98853),\n",
       " ('n', 83839),\n",
       " ('h', 80451),\n",
       " ('i', 77714),\n",
       " ('s', 77639),\n",
       " ('r', 72665),\n",
       " ('d', 53827),\n",
       " ('l', 48896),\n",
       " ('u', 38653),\n",
       " ('\\n', 35880),\n",
       " ('m', 32644),\n",
       " ('c', 31278),\n",
       " ('w', 30752),\n",
       " ('f', 26027),\n",
       " ('y', 25655),\n",
       " ('g', 22287)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freq_dict = Counter(y_train)\n",
    "char_freq_dict.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(X)\n",
    "del(y)\n",
    "del(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 726 ms, sys: 278 ms, total: 1 s\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 17:05:59.231940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-26 17:05:59.233694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:05:59.234085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:05:59.234471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:06:00.573689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:06:00.574134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:06:00.574157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-07-26 17:06:00.574533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-26 17:06:00.574602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2103 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TextVectorization(standardize = None, split = \"character\", name = 'TextVectorizer')\n",
    "vocab_json = Path(\"vocab.json\")\n",
    "\n",
    "if vocab_json.exists():\n",
    "    with vocab_json.open(\"r\") as vocab_file:\n",
    "        vocab = json.load(vocab_file)[\"vocab\"]\n",
    "    \n",
    "    vectorizer.set_vocabulary(vocab)\n",
    "else:\n",
    "    vectorizer.adapt(X_train)\n",
    "    vocab = vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "    with vocab_json.open(\"w\") as vocab_file:\n",
    "        json.dump({\"vocab\": vocab}, vocab_file)\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "char_count = len(vocab)\n",
    "char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.56 s, sys: 48.5 ms, total: 6.61 s\n",
      "Wall time: 6.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1674000,), (88106,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train = vectorizer(y_train).numpy().flatten()\n",
    "y_test = vectorizer(y_test).numpy().flatten()\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
       " 26157)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_tensor_slices((X_train, y_train)).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "train_ds, len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
       " 1377)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = Dataset.from_tensor_slices((X_test, y_test)).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "val_ds, len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " TextVectorizer (TextVectori  (None, None)             0         \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " EmbeddingLayer (Embedding)  (None, None, 32)          3456      \n",
      "                                                                 \n",
      " LSTM_1 (LSTM)               (None, None, 128)         82432     \n",
      "                                                                 \n",
      " LSTM_2 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " LSTM_3 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " LR_1 (LeakyReLU)            (None, 128)               0         \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " LR_2 (LeakyReLU)            (None, 128)               0         \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " LR_3 (LeakyReLU)            (None, 128)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 107)               13803     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 412,395\n",
      "Trainable params: 412,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_lstm_model(char_count: int, embedding_dim: int = 32):\n",
    "    input_layer = tf.keras.Input(shape = (1,), dtype = tf.string, name = 'Input')\n",
    "\n",
    "    vectorizer_layer = vectorizer(input_layer)\n",
    "    embedding_layer = Embedding(char_count + 1, embedding_dim, name = 'EmbeddingLayer')(vectorizer_layer)\n",
    "\n",
    "    lstm_1 = LSTM(128, return_sequences = True, dropout = 0.05, recurrent_dropout = 0.05, name = 'LSTM_1')(embedding_layer)\n",
    "    lstm_2 = LSTM(128, return_sequences = True, dropout = 0.05, recurrent_dropout = 0.05, name = 'LSTM_2')(lstm_1)\n",
    "    lstm_3 = LSTM(128, dropout = 0.05, recurrent_dropout = 0.05, name = 'LSTM_3')(lstm_2)\n",
    "\n",
    "    dense_1 = Dense(128, name = 'Dense_1')(lstm_3)\n",
    "    lr_1 = LeakyReLU(name = 'LR_1')(dense_1)\n",
    "    dropout_1 = Dropout(0.1, name = 'Dropout_1')(lr_1)\n",
    "\n",
    "    dense_2 = Dense(128, name = 'Dense_2')(dropout_1)\n",
    "    lr_2 = LeakyReLU(name = 'LR_2')(dense_2)\n",
    "    dropout_2 = Dropout(0.1, name = 'Dropout_2')(lr_2)\n",
    "\n",
    "    dense_3 = Dense(128, name = 'Dense_3')(dropout_2)\n",
    "    lr_3 = LeakyReLU(name = 'LR_3')(dense_3)\n",
    "\n",
    "    output_layer = Dense(char_count, activation = 'softmax', name = \"Output\")(lr_3)\n",
    "\n",
    "    model = tf.keras.Model(inputs  = input_layer, outputs = output_layer)\n",
    "    model.compile(optimizer = Adam(LR), loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_lstm_model(char_count, EMBEDDING_DIM)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 217s 21s/step - loss: 4.4011 - sparse_categorical_accuracy: 0.0859 - val_loss: 3.5621 - val_sparse_categorical_accuracy: 0.2000\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 200s 20s/step - loss: 3.3809 - sparse_categorical_accuracy: 0.1813 - val_loss: 3.2023 - val_sparse_categorical_accuracy: 0.1656\n",
      "CPU times: user 9min 26s, sys: 6min 26s, total: 15min 53s\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs = EPOCHS, steps_per_epoch = 10, validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 133s 1s/step - loss: 3.2579 - sparse_categorical_accuracy: 0.1786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.257868766784668, 0.17859375476837158]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "could only be a seaman who had been with him on\n",
      " the _Sea Unicorn_. So far as I could learn he had sailed in no\n",
      " other ship. I spent three days in wiring to Dundee, and at the\n",
      " end of that time I had ascertained the names of the crew of the\n",
      " _Sea Unicorn_ in 1883. When I found Patrick Cairns among the\n",
      " harpooners, my research was nearing its end. I argued that the\n",
      " man was probably in London, and that he would desire to leave the\n",
      " country for a time. I therefore spent some days in the East End,\n",
      " devised an \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2496999d7b744a229bccbe4b9eff7d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting chars:   0%|          | 0/50 [00:00<?, ? chars/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "                                                  \n",
      "CPU times: user 42.1 s, sys: 6.33 s, total: 48.4 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_input = random.choice(X_train)\n",
    "print(f\"Input:\\n{sample_input}\")\n",
    "\n",
    "pred_output = ''\n",
    "\n",
    "for i in trange(50, desc = \"Predicting chars\", unit = \" chars\"):\n",
    "    pred = model.predict([sample_input], verbose = False)\n",
    "    pred_char_id = pred.argmax()\n",
    "    pred_char = vocab[pred_char_id]\n",
    "    pred_output += pred_char\n",
    "    sample_input = sample_input[1:] + pred_char\n",
    "\n",
    "print(f\"Output:\\n{pred_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
